{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras import backend as K\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import pandas\n",
    "import optparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing training dataset...\n"
     ]
    }
   ],
   "source": [
    "print(\"Preparing training dataset...\")\n",
    "dataframe = pandas.read_csv('training.csv', engine='python', quotechar='|', header=None)\n",
    "dataset = dataframe.sample(frac=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['{\"username\": \"Morgan Jones\", \"project_slug\": \"protect ExwQYfgX provide smile 64741538 hJlQYFHP\", \"analysis_slug\": \"civil 40kY4OId white piEMltci ask\", \"previous_crawl\": \"81250084\", \"firstname\": \"Tanya\"}',\n",
       "        1],\n",
       "       ['{\"spreadsheetId\": \"90764359\"}', 0],\n",
       "       ['{\"packageName\": \"OwPV9DmH FAZKmGAg project rise\"}', 0],\n",
       "       ...,\n",
       "       ['{\"entryType\": \"Artist\"}', 0],\n",
       "       ['{\"zone\": \"gas there 55996324 RfSwbM3B uGfxHsVJ\", \"project\": \"table 05304138 month gun\", \"filter\": \"economy c1snemOP bkvgEcnF before 63275168\", \"orderBy\": \"group great bag TVLGCHvE 45676312\"}',\n",
       "        0],\n",
       "       ['{\"id\": \"article K83icNGl kNKOXMLs teach available 16135270\"}',\n",
       "        0]], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess dataset\n",
    "X = dataset[:,0]\n",
    "Y = dataset[:,1]\n",
    "\n",
    "for index, item in enumerate(X):\n",
    "    # Quick hack to space out json elements\n",
    "    reqJson = json.loads(item, object_pairs_hook=OrderedDict)\n",
    "    X[index] = json.dumps(reqJson, separators=(',', ':'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(filters='\\t\\n', char_level=True)\n",
    "tokenizer.fit_on_texts(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Paths\n",
    "basedir = os.path.join(os.getcwd(), os.pardir)\n",
    "out_folder = '%s/out' % basedir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and save word dictionary\n",
    "word_dict_file = ('%s/build/word-dictionary.json' % out_folder)\n",
    "\n",
    "if not os.path.exists(os.path.dirname(word_dict_file)):\n",
    "    os.makedirs(os.path.dirname(word_dict_file))\n",
    "\n",
    "with open(word_dict_file, 'w') as outfile:\n",
    "    json.dump(tokenizer.word_index, outfile, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tokenizer\n",
    "tokenizer_file = '%s/build/tokenizer.pkl' % out_folder\n",
    "if not os.path.exists(os.path.dirname(tokenizer_file)):\n",
    "    os.makedirs(os.path.dirname(tokenizer_file))\n",
    "\n",
    "with open(tokenizer_file, 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = len(tokenizer.word_index)+1\n",
    "X = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "max_payload_length = 1024\n",
    "train_size = int(len(dataset) * .75)\n",
    "\n",
    "X_processed = sequence.pad_sequences(X, maxlen=max_payload_length)\n",
    "X_train, X_test = np.array(X_processed[0:train_size], dtype=np.float), np.array(X_processed[train_size:len(X_processed)], dtype=np.float)\n",
    "Y_train, Y_test = np.array(Y[0:train_size], dtype=np.float), np.array(Y[train_size:len(Y)], dtype=np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset ready.\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 1024, 32)          1888      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1024, 32)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 26,785\n",
      "Trainable params: 26,785\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# prepare model\n",
    "has_gpu = None\n",
    "print(\"Training dataset ready.\")\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words, 32, input_length=max_payload_length))\n",
    "model.add(Dropout(0.5))\n",
    "if has_gpu is not None:\n",
    "    model.add(CuDNNLSTM(64))\n",
    "else:\n",
    "    model.add(LSTM(64, recurrent_dropout=0.5))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185/185 [==============================] - 209s 1s/step - loss: 0.4553 - accuracy: 0.8017 - val_loss: 0.3944 - val_accuracy: 0.8654\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1676e8d30>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model Train\n",
    "model.fit(X_train, Y_train, validation_split=0.25, epochs=1, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185/185 [==============================] - 191s 1s/step - loss: 0.5057 - accuracy: 0.7596 - val_loss: 0.6539 - val_accuracy: 0.6347\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1604eca60>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model Train\n",
    "model.fit(Xv2, Yv2, validation_split=0.25, epochs=1, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xv2 = np.array(X_train, dtype=np.float)\n",
    "Yv2 = np.array(Y_train, dtype=np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31413, 1024) (31413,)\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, Y_train.shape)\n",
    "print(type(X_train), type(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31413, 1024)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(Xv2.shape)\n",
    "print(type(Xv2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10472, 1024) (10472,)\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape, Y_test.shape)\n",
    "print(type(X_test), type(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt2 = np.array(X_test, dtype=np.float)\n",
    "Yt2 = np.array(Y_test, dtype=np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82/82 [==============================] - 10s 125ms/step - loss: 0.6529 - accuracy: 0.6361\n"
     ]
    }
   ],
   "source": [
    "# Model Evaluate\n",
    "score, acc = model.evaluate(Xt2, Yt2, verbose=1, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82/82 [==============================] - 10s 124ms/step - loss: 0.3901 - accuracy: 0.8700\n"
     ]
    }
   ],
   "source": [
    "# Model Evaluate\n",
    "score, acc = model.evaluate(X_test, Y_test, verbose=1, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 87.00%\n"
     ]
    }
   ],
   "source": [
    "print(\"Model Accuracy: {:0.2f}%\".format(acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save_weights('%s/privapi-lstm-weights.h5' % out_folder)\n",
    "model.save('%s/privapi-lstm-model.h5' % out_folder)\n",
    "with open('%s/privapi-lstm-model.json' % out_folder, 'w') as outfile:\n",
    "    outfile.write(model.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from collections import OrderedDict\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '%s/out' % basedir\n",
    "requests_dir = '%s/predict' % basedir\n",
    "predictions_csv = '%s/predictions.csv' % basedir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model...\n",
      "Model Loaded.\n",
      "Generating predictions...\n",
      "Predictions Generated.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Model...\")\n",
    "with open(('%s/build/tokenizer.pkl' % input_dir), 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "model = load_model('%s/privapi-lstm-model.h5' % input_dir)\n",
    "model.load_weights('%s/privapi-lstm-weights.h5' % input_dir)\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "print(\"Model Loaded.\")\n",
    "print(\"Generating predictions...\")\n",
    "result_dict = []\n",
    "for dirpath, dirs, files in os.walk(requests_dir):\n",
    "    reqs = [fi for fi in files if fi.endswith(\".json\")]\n",
    "    for req in reqs:\n",
    "        reqf = os.path.join(dirpath, req)\n",
    "        with open(reqf) as f:\n",
    "            reqd = json.load(f, object_pairs_hook=OrderedDict)\n",
    "            reqj = json.dumps(reqd, separators=(',', ':'))\n",
    "            reqs = tokenizer.texts_to_sequences([reqj])\n",
    "            max_log_length = 1024\n",
    "            reqsp = sequence.pad_sequences(reqs, maxlen=max_log_length)\n",
    "            prediction = model.predict(reqsp)\n",
    "            prediction_class = model.predict_classes(reqsp)\n",
    "            result_dict.append([os.path.basename(reqf), prediction_class[0][0], prediction[0][0]])\n",
    "\n",
    "result_array = np.array(result_dict)\n",
    "df = pd.DataFrame(result_array)\n",
    "df.columns = ['payload_file', 'is_sensitive', 'probability']\n",
    "df.to_csv(predictions_csv, index=False)\n",
    "print(\"Predictions Generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip show tensorflow\n",
    "pandas\n",
    "faker\n",
    "faker-credit-score\n",
    "pyswagger\n",
    "requests\n",
    "keras\n",
    "simplejson\n",
    "xeger\n",
    "numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                Version\n",
      "---------------------- ---------\n",
      "absl-py                0.10.0\n",
      "appnope                0.1.0\n",
      "astroid                2.4.2\n",
      "astunparse             1.6.3\n",
      "attrs                  19.3.0\n",
      "autograd               1.3\n",
      "backcall               0.2.0\n",
      "beautifulsoup4         4.9.1\n",
      "bleach                 3.1.5\n",
      "bs4                    0.0.1\n",
      "cachetools             4.1.1\n",
      "certifi                2020.6.20\n",
      "chardet                3.0.4\n",
      "colorama               0.4.3\n",
      "configparser           5.0.0\n",
      "crayons                0.3.1\n",
      "cycler                 0.10.0\n",
      "decorator              4.4.2\n",
      "defusedxml             0.6.0\n",
      "dill                   0.3.2\n",
      "entrypoints            0.3\n",
      "fake-useragent         0.1.11\n",
      "Faker                  1.0.2\n",
      "faker-credit-score     0.2.2\n",
      "future                 0.18.2\n",
      "gast                   0.3.3\n",
      "GDAL                   3.1.2\n",
      "google-auth            1.21.2\n",
      "google-auth-oauthlib   0.4.1\n",
      "google-pasta           0.2.0\n",
      "grpcio                 1.32.0\n",
      "h5py                   2.10.0\n",
      "html5lib               1.1\n",
      "idna                   2.8\n",
      "ipykernel              5.3.4\n",
      "ipython                7.16.1\n",
      "ipython-genutils       0.2.0\n",
      "isort                  5.5.1\n",
      "jedi                   0.17.2\n",
      "Jinja2                 2.11.2\n",
      "json5                  0.9.5\n",
      "jsonschema             3.2.0\n",
      "jupyter-client         6.1.6\n",
      "jupyter-core           4.6.3\n",
      "jupyter-kite           1.0.0\n",
      "jupyterlab             2.2.2\n",
      "jupyterlab-server      1.2.0\n",
      "Keras                  2.2.4\n",
      "Keras-Applications     1.0.8\n",
      "Keras-Preprocessing    1.1.2\n",
      "kiwisolver             1.2.0\n",
      "lazy-object-proxy      1.4.3\n",
      "Lifetimes              0.11.3\n",
      "lxml                   4.5.2\n",
      "Markdown               3.2.2\n",
      "MarkupSafe             1.1.1\n",
      "matplotlib             3.3.0\n",
      "mccabe                 0.6.1\n",
      "mistune                0.8.4\n",
      "nbconvert              5.6.1\n",
      "nbformat               5.0.7\n",
      "notebook               6.0.3\n",
      "numpy                  1.19.2\n",
      "oauthlib               3.1.0\n",
      "opt-einsum             3.3.0\n",
      "packaging              20.4\n",
      "pandas                 0.24.1\n",
      "pandocfilters          1.4.2\n",
      "parso                  0.7.1\n",
      "pexpect                4.8.0\n",
      "pickleshare            0.7.5\n",
      "Pillow                 7.2.0\n",
      "pip                    20.2.3\n",
      "prometheus-client      0.8.0\n",
      "prompt-toolkit         3.0.5\n",
      "protobuf               3.12.4\n",
      "psycopg2               2.8.5\n",
      "ptyprocess             0.6.0\n",
      "pyaml                  20.4.0\n",
      "pyasn1                 0.4.8\n",
      "pyasn1-modules         0.2.8\n",
      "Pygments               2.6.1\n",
      "pylint                 2.6.0\n",
      "pymongo                3.11.0\n",
      "PyMySQL                0.10.0\n",
      "pyparsing              2.4.7\n",
      "pyrsistent             0.16.0\n",
      "pyswagger              0.8.39\n",
      "python-dateutil        2.8.1\n",
      "pytz                   2020.1\n",
      "PyYAML                 5.3.1\n",
      "pyzmq                  19.0.1\n",
      "regex                  2020.7.14\n",
      "requests               2.21.0\n",
      "requests-oauthlib      1.3.0\n",
      "rsa                    4.6\n",
      "scipy                  1.4.1\n",
      "seaborn                0.10.1\n",
      "selenium               3.141.0\n",
      "Send2Trash             1.5.0\n",
      "setuptools             49.2.0\n",
      "simplejson             3.16.0\n",
      "six                    1.15.0\n",
      "soupsieve              2.0.1\n",
      "SQLAlchemy             1.3.19\n",
      "tensorboard            2.3.0\n",
      "tensorboard-plugin-wit 1.7.0\n",
      "tensorflow             2.3.0\n",
      "tensorflow-estimator   2.3.0\n",
      "termcolor              1.1.0\n",
      "terminado              0.8.3\n",
      "testpath               0.4.4\n",
      "text-unidecode         1.2\n",
      "toml                   0.10.1\n",
      "tornado                6.0.4\n",
      "traitlets              4.3.3\n",
      "urllib3                1.24.3\n",
      "useragent              0.1.1\n",
      "validate-email         1.3\n",
      "wcwidth                0.2.5\n",
      "webdriver-manager      3.2.1\n",
      "webencodings           0.5.1\n",
      "Werkzeug               1.0.1\n",
      "wheel                  0.34.2\n",
      "wrapt                  1.12.1\n",
      "xeger                  0.3.4\n"
     ]
    }
   ],
   "source": [
    "! pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"train.py\", line 3, in <module>\r\n",
      "    from tensorflow.keras.models import Sequential\r\n",
      "ImportError: No module named 'tensorflow'\r\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
